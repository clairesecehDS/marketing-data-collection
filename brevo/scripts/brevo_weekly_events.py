#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de r√©cup√©ration des √©v√©nements marketing hebdomadaires de Brevo
Documentation: https://developers.brevo.com/docs/fetch-all-your-weekly-marketing-events

Ce script permet de:
1. D√©clencher un export d'√©v√©nements via l'API Brevo
2. V√©rifier le statut de l'export
3. T√©l√©charger le fichier CSV g√©n√©r√©
4. Parser et uploader les donn√©es vers BigQuery
"""

import os
import sys
import time
import yaml
import requests
import csv
import io
import zipfile
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BrevoWeeklyEventsCollector:
    """Collecteur d'√©v√©nements marketing hebdomadaires Brevo"""
    
    def __init__(self, api_key: str, config: dict):
        """
        Initialise le collecteur Brevo
        
        Args:
            api_key: Cl√© API Brevo
            config: Configuration compl√®te du projet
        """
        self.api_key = api_key
        self.config = config
        self.base_url = "https://api.brevo.com/v3"
        self.headers = {
            "accept": "application/json",
            "content-type": "application/json",
            "api-key": self.api_key
        }
        
    def request_export(self, event_type: str = "allEvents", days: int = 7, notify_url: str = None) -> Optional[int]:
        """
        D√©clenche un export d'√©v√©nements webhooks
        
        Args:
            event_type: Type d'√©v√©nement √† exporter (allEvents, spam, opened, click, etc.)
            days: Nombre de jours √† exporter (max 7)
            notify_url: URL du webhook pour recevoir la notification quand l'export est pr√™t
            
        Returns:
            process_id si succ√®s, None sinon
        """
        url = f"{self.base_url}/webhooks/export"
        
        payload = {
            "event": event_type,
            "type": "marketing",
            "days": days
        }
        
        # Ajouter le notifyURL si fourni
        if notify_url:
            payload["notifyURL"] = notify_url
            logger.info(f"Webhook de notification: {notify_url}")
        
        try:
            logger.info(f"Demande d'export pour {event_type} sur {days} jours...")
            response = requests.post(url, json=payload, headers=self.headers)
            response.raise_for_status()
            
            data = response.json()
            process_id = data.get("processId")
            
            if process_id:
                logger.info(f"‚úì Export demand√© avec succ√®s. Process ID: {process_id}")
                return process_id
            else:
                logger.error("Process ID non trouv√© dans la r√©ponse")
                return None
                
        except requests.exceptions.RequestException as e:
            logger.error(f"‚úó Erreur lors de la demande d'export: {e}")
            if hasattr(e.response, 'text'):
                logger.error(f"R√©ponse API: {e.response.text}")
            return None
    
    def check_export_status(self, process_id: int) -> Optional[Dict]:
        """
        V√©rifie le statut d'un export
        
        Args:
            process_id: ID du processus d'export
            
        Returns:
            Informations sur le processus si trouv√©, None sinon
        """
        url = f"{self.base_url}/processes/{process_id}"
        
        try:
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()
            
            return response.json()
            
        except requests.exceptions.RequestException as e:
            logger.error(f"‚úó Erreur lors de la v√©rification du statut: {e}")
            return None
    
    def wait_for_export(self, process_id: int, max_wait_seconds: int = 600, check_interval: int = 10) -> Optional[str]:
        """
        Attend que l'export soit termin√© et retourne l'URL de t√©l√©chargement
        
        Args:
            process_id: ID du processus d'export
            max_wait_seconds: Temps d'attente maximum en secondes
            check_interval: Intervalle entre chaque v√©rification en secondes
            
        Returns:
            URL de t√©l√©chargement si succ√®s, None sinon
        """
        start_time = time.time()
        
        while time.time() - start_time < max_wait_seconds:
            status_info = self.check_export_status(process_id)
            
            if not status_info:
                logger.error("Impossible de r√©cup√©rer le statut")
                return None
            
            status = status_info.get("status")
            logger.info(f"Statut de l'export: {status}")
            
            if status == "completed":
                export_url = status_info.get("export_url")
                if export_url:
                    logger.info(f"‚úì Export termin√©! URL: {export_url}")
                    return export_url
                else:
                    logger.error("Export termin√© mais URL non trouv√©e")
                    return None
            
            elif status == "failed":
                logger.error("‚úó Export √©chou√©")
                return None
            
            logger.info(f"En attente... (v√©rifie √† nouveau dans {check_interval}s)")
            time.sleep(check_interval)
        
        logger.error(f"‚úó Timeout apr√®s {max_wait_seconds}s")
        return None
    
    def download_and_parse_export(self, export_url: str) -> List[Dict]:
        """
        T√©l√©charge et parse le fichier CSV d'export
        
        Args:
            export_url: URL du fichier √† t√©l√©charger
            
        Returns:
            Liste de dictionnaires contenant les √©v√©nements
        """
        try:
            logger.info("T√©l√©chargement du fichier d'export...")
            response = requests.get(export_url)
            response.raise_for_status()
            
            events = []
            
            # V√©rifier si c'est un fichier ZIP
            if export_url.endswith('.zip'):
                logger.info("Fichier ZIP d√©tect√©, extraction...")
                with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:
                    # Traiter tous les fichiers CSV dans le ZIP
                    for file_name in zip_file.namelist():
                        if file_name.endswith('.csv'):
                            logger.info(f"Traitement du fichier: {file_name}")
                            with zip_file.open(file_name) as csv_file:
                                csv_content = csv_file.read().decode('utf-8')
                                events.extend(self._parse_csv_content(csv_content))
            else:
                # Fichier CSV direct
                logger.info("Traitement du fichier CSV...")
                events = self._parse_csv_content(response.text)
            
            logger.info(f"‚úì {len(events)} √©v√©nements extraits")
            return events
            
        except Exception as e:
            logger.error(f"‚úó Erreur lors du t√©l√©chargement/parsing: {e}")
            return []
    
    def _parse_csv_content(self, csv_content: str) -> List[Dict]:
        """
        Parse le contenu CSV et retourne une liste de dictionnaires
        
        Args:
            csv_content: Contenu du CSV sous forme de string
            
        Returns:
            Liste de dictionnaires
        """
        events = []
        csv_reader = csv.DictReader(io.StringIO(csv_content))
        
        for row in csv_reader:
            # Convertir les champs en format appropri√© pour BigQuery
            event = self._transform_row(row)
            events.append(event)
        
        return events
    
    def _transform_row(self, row: Dict) -> Dict:
        """
        Transforme une ligne CSV en format BigQuery
        
        Args:
            row: Ligne CSV sous forme de dictionnaire
            
        Returns:
            Dictionnaire format√© pour BigQuery
        """
        # Parser le champ event pour cr√©er les compteurs
        event_type = row.get('event', '').strip()
        
        # Mapping des √©v√©nements vers les colonnes bool√©ennes
        event_mapping = {
            'spam': 'spam',
            'opened': 'opened',
            'click': 'click',
            'hard_bounce': 'hard_bounce',
            'soft_bounce': 'soft_bounce',
            'delivered': 'delivered',
            'unsubscribe': 'unsubscribe',
            'contact_deleted': 'contact_deleted',
            'contact_updated': 'contact_updated',
            'list_addition': 'list_addition'
        }
        
        # Cr√©er l'objet transform√©
        transformed = {
            'date': row.get('date'),
            'email': row.get('email'),
            'event': event_type,
            'id': int(row.get('id', 0)) if row.get('id') else None,
            'message_id': row.get('message-id'),
            'reason': row.get('reason'),
            'sending_ip': row.get('sending_ip'),
            'subject': row.get('subject'),
            'tag': row.get('tag'),
            'tags': row.get('tags'),
            'template_id': int(row.get('template_id', 0)) if row.get('template_id') else None,
            'ts': row.get('ts'),
            'ts_epoch': int(row.get('ts_epoch', 0)) if row.get('ts_epoch') else None,
            'ts_event': row.get('ts_event'),
            'x_mailin_custom': row.get('X-Mailin-custom'),
            'link': row.get('link'),
            's_returnpath': row.get('s_returnpath', '').lower() == 'true',
            'retrieved_at': datetime.utcnow().isoformat(),
            'export_process_id': None  # Sera rempli lors de l'upload
        }
        
        # Ajouter les compteurs d'√©v√©nements
        for event_key, column_name in event_mapping.items():
            transformed[column_name] = 1 if event_type == event_key else 0
        
        return transformed
    
    def save_to_json(self, events: List[Dict], output_file: str):
        """
        Sauvegarde les √©v√©nements dans un fichier JSON
        
        Args:
            events: Liste des √©v√©nements
            output_file: Chemin du fichier de sortie
        """
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(events, f, indent=2, ensure_ascii=False)
            logger.info(f"‚úì Donn√©es sauvegard√©es dans {output_file}")
        except Exception as e:
            logger.error(f"‚úó Erreur lors de la sauvegarde: {e}")
    
    def upload_to_bigquery(self, events: List[Dict], process_id: int):
        """
        Upload les √©v√©nements vers BigQuery
        
        Args:
            events: Liste des √©v√©nements
            process_id: ID du processus d'export
        """
        try:
            from google.cloud import bigquery
            from google.oauth2 import service_account
            
            # Configuration BigQuery
            gcp_config = self.config.get('google_cloud', {})
            project_id = gcp_config.get('project_id')
            credentials_file = gcp_config.get('credentials_file')
            dataset_id = gcp_config.get('datasets', {}).get('brevo', 'brevo')
            table_id = 'brevo'
            
            # Cr√©er le client BigQuery
            credentials = service_account.Credentials.from_service_account_file(credentials_file)
            client = bigquery.Client(credentials=credentials, project=project_id)
            
            # R√©f√©rence de la table
            table_ref = f"{project_id}.{dataset_id}.{table_id}"
            
            # Ajouter le process_id √† chaque √©v√©nement
            for event in events:
                event['export_process_id'] = process_id
            
            # Configuration du job d'insertion
            job_config = bigquery.LoadJobConfig(
                source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
                write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
                schema_update_options=[
                    bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION
                ]
            )
            
            # Convertir en NDJSON
            ndjson_data = '\n'.join([json.dumps(event) for event in events])
            
            # Upload
            logger.info(f"Upload de {len(events)} √©v√©nements vers {table_ref}...")
            job = client.load_table_from_file(
                io.StringIO(ndjson_data),
                table_ref,
                job_config=job_config
            )
            
            job.result()  # Attendre la fin du job
            
            logger.info(f"‚úì {len(events)} √©v√©nements upload√©s avec succ√®s vers BigQuery!")
            
        except Exception as e:
            logger.error(f"‚úó Erreur lors de l'upload vers BigQuery: {e}")
            raise


def load_config(config_file: str = "config.yaml") -> dict:
    """Charge la configuration depuis le fichier YAML"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        logger.error(f"Erreur lors du chargement de la configuration: {e}")
        sys.exit(1)


def main():
    """Fonction principale"""
    # Charger la configuration
    config = load_config()
    
    # R√©cup√©rer les param√®tres Brevo
    brevo_config = config.get('brevo', {})
    api_key = brevo_config.get('api_key')
    notify_url = brevo_config.get('notify_url')
    
    if not api_key:
        logger.error("Cl√© API Brevo non trouv√©e dans la configuration")
        sys.exit(1)
    
    if not notify_url or notify_url == "https://webhook.site/YOUR-UNIQUE-ID":
        logger.error("‚ö†Ô∏è  Veuillez configurer 'notify_url' dans config.yaml")
        logger.error("   1. Allez sur https://webhook.site")
        logger.error("   2. Copiez votre URL unique")
        logger.error("   3. Collez-la dans brevo.notify_url du config.yaml")
        sys.exit(1)
    
    # Cr√©er le collecteur
    collector = BrevoWeeklyEventsCollector(api_key, config)
    
    logger.info("="*60)
    logger.info("üîî IMPORTANT: Surveillez votre webhook.site !")
    logger.info(f"   URL: {notify_url}")
    logger.info("   Brevo va y envoyer le lien de t√©l√©chargement du CSV")
    logger.info("="*60 + "\n")
    
    # 1. Demander l'export
    process_id = collector.request_export(event_type="allEvents", days=7, notify_url=notify_url)
    
    if not process_id:
        logger.error("Impossible de d√©marrer l'export")
        sys.exit(1)
    
    # 2. Instructions pour l'utilisateur
    logger.info("\n" + "="*60)
    logger.info("üìã PROCHAINES √âTAPES:")
    logger.info("="*60)
    logger.info("1. Attendez quelques minutes que Brevo g√©n√®re l'export")
    logger.info("2. Rafra√Æchissez votre page webhook.site")
    logger.info("3. Vous recevrez un JSON avec 'url' et 'process_id'")
    logger.info("4. Copiez l'URL du fichier CSV/ZIP")
    logger.info("5. Relancez ce script avec l'URL en param√®tre:")
    logger.info(f"   python {sys.argv[0]} --download-url 'URL_DU_FICHIER'")
    logger.info("="*60 + "\n")
    
    # Option: attendre et poller le statut
    logger.info("Ou attendez que le script v√©rifie automatiquement le statut...")
    export_url = collector.wait_for_export(process_id, max_wait_seconds=600)
    
    if not export_url:
        logger.warning("‚ö†Ô∏è  Timeout - V√©rifiez votre webhook.site pour r√©cup√©rer l'URL manuellement")
        logger.info(f"   Process ID: {process_id}")
        sys.exit(0)
    
    # 3. T√©l√©charger et parser les donn√©es
    events = collector.download_and_parse_export(export_url)
    
    if not events:
        logger.warning("Aucun √©v√©nement r√©cup√©r√©")
        sys.exit(0)
    
    # 4. Sauvegarder localement (optionnel)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"brevo_events_{timestamp}.json"
    collector.save_to_json(events, output_file)
    
    # 5. Upload vers BigQuery
    try:
        collector.upload_to_bigquery(events, process_id)
        logger.info("‚úì Processus termin√© avec succ√®s!")
    except Exception as e:
        logger.error(f"‚úó Erreur lors de l'upload: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
