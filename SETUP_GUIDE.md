# Marketing Data Collection Suite - Setup Guide

Documentation complÃ¨te pour la configuration et l'utilisation de la suite de collecte de donnÃ©es marketing (LinkedIn, Microsoft Clarity, SpyFu).

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [Architecture globale](#architecture-globale)
- [PrÃ©requis techniques](#prÃ©requis-techniques)
- [Configuration Google Cloud & BigQuery](#configuration-google-cloud--bigquery)
- [Configuration LinkedIn](#configuration-linkedin)
- [Configuration Microsoft Clarity](#configuration-microsoft-clarity)
- [Configuration SpyFu](#configuration-spyfu)
- [DÃ©ploiement et exÃ©cution](#dÃ©ploiement-et-exÃ©cution)
- [SÃ©curitÃ© et bonnes pratiques](#sÃ©curitÃ©-et-bonnes-pratiques)
- [Automatisation](#automatisation)
- [Troubleshooting global](#troubleshooting-global)
- [Checklist de dÃ©ploiement](#checklist-de-dÃ©ploiement)

---

## ğŸ¯ Vue d'ensemble

Cette suite permet de collecter automatiquement des donnÃ©es marketing depuis 3 sources :

| Source | Type de donnÃ©es | FrÃ©quence recommandÃ©e |
|--------|-----------------|----------------------|
| **LinkedIn** | Campagnes publicitaires, budgets, leads | Quotidien |
| **Microsoft Clarity** | Comportement utilisateur, UX metrics | Quotidien (obligatoire) |
| **SpyFu** | SEO/PPC concurrentiel, keywords | Hebdomadaire |

**Objectif :** Centraliser toutes les donnÃ©es dans BigQuery pour analyse et reporting unifiÃ©s.

### BÃ©nÃ©fices

âœ… **Centralisation** - Toutes les donnÃ©es au mÃªme endroit
âœ… **Historisation** - Construction d'un historique long terme
âœ… **Analyse croisÃ©e** - CorrÃ©lations entre sources
âœ… **Automatisation** - Collecte sans intervention manuelle
âœ… **Backup** - Sauvegarde JSON avant upload

---

## ğŸ—ï¸ Architecture globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Sources de donnÃ©es                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   LinkedIn Ads  â”‚ Microsoft Clarity â”‚      SpyFu API        â”‚
â”‚   - Campaigns   â”‚   - User behavior â”‚   - SEO keywords      â”‚
â”‚   - Budgets     â”‚   - Frustration   â”‚   - PPC keywords      â”‚
â”‚   - Lead forms  â”‚   - Engagement    â”‚   - Competitors       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                  â”‚
         â”‚ OAuth 2.0         â”‚ API Key          â”‚ API Key
         â”‚                   â”‚                  â”‚
         v                   v                  v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Scripts Python (ce repository)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  linkedin/   â”‚  â”‚  microsoft_  â”‚  â”‚   spyfu/     â”‚       â”‚
â”‚  â”‚  scripts/    â”‚  â”‚  clarity/    â”‚  â”‚   scripts/   â”‚       â”‚
â”‚  â”‚              â”‚  â”‚  scripts/    â”‚  â”‚              â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                 â”‚                 â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                           â”‚                                 â”‚
â”‚                  JSON Backup (data/)                        â”‚
â”‚                           â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ Service Account
                            â”‚
                            v
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   BigQuery      â”‚
                    â”‚   (Google Cloud)â”‚
                    â”‚                 â”‚
                    â”‚  - linkedin     â”‚
                    â”‚  - clarity      â”‚
                    â”‚  - spyfu        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             v
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Visualization  â”‚
                    â”‚  - Looker       â”‚
                    â”‚  - Tableau      â”‚
                    â”‚  - Custom SQL   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ PrÃ©requis techniques

### Comptes nÃ©cessaires

| Service | Type | CoÃ»t |
|---------|------|------|
| Google Cloud Platform | Cloud provider | Gratuit pour petits volumes, puis facturation usage |
| LinkedIn Developer | Marketing API | Gratuit (nÃ©cessite Ad Account actif) |
| Microsoft Clarity | Analytics | Totalement gratuit |
| SpyFu | SEO/PPC Intelligence | Payant - selon abonnement |

### Logiciels requis

```bash
# Python 3.8+
python --version

# Pip
pip --version

# Git (pour cloner le repository)
git --version
```

---

## ğŸ“¥ Installation du code

### Ã‰tape 1 : Cloner le repository GitHub

```bash
# Cloner le repository
git clone [URL_DU_REPO_GITHUB]

# Se dÃ©placer dans le dossier
cd marketing-data-collection

# Structure attendue aprÃ¨s clonage :
# marketing-data-collection/
# â”œâ”€â”€ linkedin/
# â”œâ”€â”€ microsoft_clarity/
# â”œâ”€â”€ spyfu/
# â”œâ”€â”€ SETUP_GUIDE.md
# â””â”€â”€ README.md
```

âš ï¸ **Important :** Le fichier `account-key.json` n'est PAS dans le repository (pour des raisons de sÃ©curitÃ©). Vous devrez le crÃ©er comme expliquÃ© dans la section "Configuration Google Cloud".

---

### Ã‰tape 2 : Configurer le fichier de configuration

```bash
# Copier le fichier d'exemple
cp config.example.yaml config.yaml

# Ã‰diter avec vos paramÃ¨tres
nano config.yaml  # ou vim, code, etc.
```

**Ce fichier centralise TOUTE la configuration :**
- Credentials (API keys, tokens, project IDs)
- Liste des domaines et concurrents Ã  analyser
- MÃ©triques Ã  collecter
- PÃ©riodes de collecte
- ParamÃ¨tres BigQuery
- Planification de l'automatisation

ğŸ“¸ **TODO: Screenshot du fichier config.yaml ouvert dans un Ã©diteur**

Voir section [Configuration dÃ©taillÃ©e](#-configuration-dÃ©taillÃ©e) pour remplir chaque partie.

---

### Ã‰tape 3 : Installer les dÃ©pendances Python

```bash
# CrÃ©er un environnement virtuel (recommandÃ©)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install requests pandas pandas-gbq google-auth google-cloud-bigquery "numpy<2.0.0"
```

---

### Ã‰tape 3 : VÃ©rifier l'installation

```bash
# VÃ©rifier que les modules sont installÃ©s
python -c "import requests, pandas, pandas_gbq, google.auth; print('âœ“ Toutes les dÃ©pendances sont installÃ©es')"
```

---

## ğŸ”§ Configuration Google Cloud & BigQuery

### Ã‰tape 1 : CrÃ©er un projet Google Cloud

1. Aller sur [Google Cloud Console](https://console.cloud.google.com/)
2. Cliquer sur le sÃ©lecteur de projet en haut
3. Cliquer sur **"Nouveau projet"**
4. Renseigner :
   - **Nom du projet** : `deepscouting-marketing` (ou autre)
   - **Organisation** : SÃ©lectionner si applicable
5. Cliquer sur **"CrÃ©er"**
6. **Noter le Project ID** (ex: `clean-avatar-466709-a0`)

---

### Ã‰tape 2 : Activer l'API BigQuery

1. Dans la console GCP, menu latÃ©ral â†’ **"APIs & Services"** â†’ **"Library"**
2. Rechercher **"BigQuery API"**
3. Cliquer sur **"Enable"**

---

### Ã‰tape 3 : CrÃ©er un Service Account

Un Service Account permet aux scripts d'accÃ©der Ã  BigQuery de maniÃ¨re sÃ©curisÃ©e.

1. Menu latÃ©ral â†’ **"IAM & Admin"** â†’ **"Service Accounts"**
2. Cliquer sur **"Create Service Account"**
3. Renseigner :
   - **Service account name** : `marketing-data-collector`
   - **Description** : "Service account for automated data collection"
4. Cliquer sur **"Create and Continue"**

5. **Accorder les permissions** :
   - RÃ´le : **"BigQuery Data Editor"**
   - RÃ´le : **"BigQuery Job User"**
   - Cliquer sur **"Continue"**

6. Cliquer sur **"Done"**

---

### Ã‰tape 4 : GÃ©nÃ©rer une clÃ© JSON

1. Dans la liste des Service Accounts, cliquer sur le service account crÃ©Ã©
2. Onglet **"Keys"**
3. Cliquer sur **"Add Key"** â†’ **"Create new key"**
4. SÃ©lectionner **JSON**
5. Cliquer sur **"Create"**
6. Un fichier JSON se tÃ©lÃ©charge automatiquement

âš ï¸ **Important :** Ce fichier contient des credentials sensibles !

7. **Renommer le fichier** en `account-key.json`
8. **Placer le fichier** Ã  la racine du repository clonÃ© :
   ```bash
   # Si vous avez clonÃ© dans /home/user/marketing-data-collection/
   mv ~/Downloads/account-key-xxx.json /home/user/marketing-data-collection/account-key.json
   ```

9. **SÃ©curiser le fichier** :
   ```bash
   chmod 600 account-key.json
   ```

âš ï¸ **Ne jamais commiter ce fichier dans Git !** Il est dÃ©jÃ  dans le `.gitignore`.

---

### Ã‰tape 5 : CrÃ©er les datasets BigQuery

1. Menu latÃ©ral â†’ **"BigQuery"** â†’ **"SQL Workspace"**
2. Cliquer sur votre projet
3. Cliquer sur les trois points â†’ **"Create dataset"**

**CrÃ©er 3 datasets :**

#### Dataset 1 : LinkedIn
- **Dataset ID** : `linkedin`
- **Data location** : `EU` (ou US selon votre rÃ©gion)
- **Default table expiration** : Never
- Cliquer sur **"Create dataset"**

#### Dataset 2 : Microsoft Clarity
- **Dataset ID** : `microsoft_clarity`
- **Data location** : `EU`
- **Default table expiration** : Never

#### Dataset 3 : SpyFu
- **Dataset ID** : `spyfu`
- **Data location** : `EU`
- **Default table expiration** : Never

---

### Ã‰tape 6 : CrÃ©er les tables et vues

Pour chaque source, exÃ©cuter les fichiers SQL :

#### Via bq CLI (recommandÃ©)

```bash
# Installer gcloud CLI si nÃ©cessaire
# https://cloud.google.com/sdk/docs/install

# Authentification
gcloud auth login
gcloud config set project VOTRE_PROJECT_ID

# CrÃ©er les tables LinkedIn
bq query --use_legacy_sql=false < linkedin/sql/bigquery_campaign_creative_schema.sql
bq query --use_legacy_sql=false < linkedin/sql/bigquery_campaign_creative_budget_schema.sql
bq query --use_legacy_sql=false < linkedin/sql/bigquery_lead_forms_schema.sql

# CrÃ©er la table Clarity
bq query --use_legacy_sql=false < microsoft_clarity/sql/bigquery_clarity_schema.sql

# CrÃ©er les tables SpyFu
bq query --use_legacy_sql=false < spyfu/sql/bigquery_spyfu_schema.sql
```

#### Via Console BigQuery

1. Aller dans BigQuery SQL Workspace
2. Cliquer sur **"Compose new query"**
3. Copier-coller le contenu d'un fichier SQL
4. Cliquer sur **"Run"**
5. RÃ©pÃ©ter pour chaque fichier SQL

---

### Ã‰tape 7 : Connecter Google Search Console, Google Analytics et Google Ads (NATIF)

BigQuery peut se connecter **directement** aux services Google sans code ni script ! Les donnÃ©es sont synchronisÃ©es automatiquement.

---

#### 7.1 - Google Search Console â†’ BigQuery

**Export NATIF des donnÃ©es Search Console vers BigQuery**

1. Aller sur [Google Search Console](https://search.google.com/search-console)
2. SÃ©lectionner votre propriÃ©tÃ© (site web)
3. Menu latÃ©ral â†’ **"ParamÃ¨tres"** (âš™ï¸)
4. Section **"Exportations de donnÃ©es"** â†’ Cliquer sur **"BigQuery"**
5. Cliquer sur **"Exporter vers BigQuery"**
6. Configurer :
   - **Projet Google Cloud** : SÃ©lectionner votre projet GCP
   - **Dataset** : CrÃ©er un nouveau dataset `google_search_console` ou utiliser existant
   - **FrÃ©quence** : Quotidienne (automatique)
   - **DonnÃ©es historiques** : Cocher si vous voulez les 16 derniers mois
7. Cliquer sur **"Exporter"**


**RÃ©sultat :**
- Table crÃ©Ã©e automatiquement : `searchdata_site_impression`
- Mise Ã  jour quotidienne automatique
- Colonnes : url, query, impressions, clicks, position, country, device, etc.

**Exemple de requÃªte :**

```sql
SELECT
  data_date,
  query,
  SUM(impressions) as total_impressions,
  SUM(clicks) as total_clicks,
  AVG(sum_position / impressions) as avg_position
FROM `votre-project-id.google_search_console.searchdata_site_impression`
WHERE data_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
GROUP BY data_date, query
ORDER BY total_clicks DESC
LIMIT 100;
```

---

#### 7.2 - Google Analytics 4 â†’ BigQuery

**Export NATIF GA4 vers BigQuery (gratuit jusqu'Ã  1M Ã©vÃ©nements/jour)**

1. Aller dans [Google Analytics 4](https://analytics.google.com/)
2. Menu **"Admin"** (âš™ï¸ en bas Ã  gauche)
3. Colonne **"PropriÃ©tÃ©"** â†’ **"BigQuery Linking"** ou **"Lien BigQuery"**
4. Cliquer sur **"Link"** ou **"Associer"**
5. SÃ©lectionner votre projet Google Cloud
6. Configurer :
   - **Dataset location** : Choisir `EU` ou `US`
   - **Nom du dataset** : `analytics_XXXXXXXXX` (auto-gÃ©nÃ©rÃ©)
   - **FrÃ©quence d'export** :
     - âœ… **Quotidien** (Daily) - Export tous les jours Ã  ~14h
     - âœ… **Streaming** (si besoin temps rÃ©el, payant au-delÃ  de 1M Ã©vÃ©nements/jour)
   - **Inclure les donnÃ©es publicitaires** : Cocher si applicable
7. Cliquer sur **"Suivant"** â†’ **"Envoyer"**

**RÃ©sultat :**
- Tables crÃ©Ã©es automatiquement : `events_YYYYMMDD` (une par jour)
- Table intraday : `events_intraday_YYYYMMDD` (si streaming activÃ©)
- SchÃ©ma nested avec Ã©vÃ©nements, paramÃ¨tres, user properties

**Exemple de requÃªte - Sessions et utilisateurs :**

```sql
SELECT
  PARSE_DATE('%Y%m%d', event_date) as date,
  COUNT(DISTINCT user_pseudo_id) as users,
  COUNT(DISTINCT CONCAT(user_pseudo_id,
    (SELECT value.int_value FROM UNNEST(event_params)
     WHERE key = 'ga_session_id'))) as sessions
FROM `votre-project-id.analytics_XXXXXXXXX.events_*`
WHERE _TABLE_SUFFIX BETWEEN '20250101' AND '20251231'
GROUP BY date
ORDER BY date DESC;
```

**Exemple de requÃªte - Top pages :**

```sql
SELECT
  (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') as page_url,
  COUNT(*) as page_views,
  COUNT(DISTINCT user_pseudo_id) as unique_users
FROM `votre-project-id.analytics_XXXXXXXXX.events_*`
WHERE event_name = 'page_view'
  AND _TABLE_SUFFIX BETWEEN '20250101' AND '20251231'
GROUP BY page_url
ORDER BY page_views DESC
LIMIT 20;
```

---

#### 7.3 - Google Ads â†’ BigQuery

**Export NATIF Google Ads vers BigQuery**

âš ï¸ **PrÃ©requis :** Compte Google Ads avec droits administrateur

1. Se connecter Ã  [Google Ads](https://ads.google.com/)
2. Menu **"Outils et paramÃ¨tres"** (ğŸ”§) â†’ **"Configuration"** â†’ **"Exports BigQuery"**
3. Cliquer sur le bouton **+** (Nouvel export)
4. Configurer :
   - **Projet Google Cloud** : SÃ©lectionner votre projet
   - **Dataset** : CrÃ©er `google_ads` ou utiliser existant
   - **Emplacement des donnÃ©es** : `EU` ou `US`
   - **Tables Ã  exporter** : SÃ©lectionner les tables nÃ©cessaires
     - âœ… Campaign (campagnes)
     - âœ… Ad Group (groupes d'annonces)
     - âœ… Ad (annonces)
     - âœ… Keyword (mots-clÃ©s)
     - âœ… Search Term (termes de recherche)
     - âœ… Geo (gÃ©ographie)
     - âœ… Click (clics)
     - Etc.
   - **FrÃ©quence** : Quotidienne (automatique chaque nuit)
5. Cliquer sur **"CrÃ©er"**

**RÃ©sultat :**
- Tables crÃ©Ã©es : `Campaign_XXXXXXXX`, `AdGroup_XXXXXXXX`, etc.
- Mise Ã  jour quotidienne automatique
- Historique : jusqu'Ã  13 mois de donnÃ©es

**Exemple de requÃªte - Performances campagnes :**

```sql
SELECT
  c.campaign_name,
  SUM(c.impressions) as total_impressions,
  SUM(c.clicks) as total_clicks,
  SUM(c.conversions) as total_conversions,
  SUM(c.cost_micros) / 1000000 as total_cost_usd,
  SAFE_DIVIDE(SUM(c.clicks), SUM(c.impressions)) * 100 as ctr,
  SAFE_DIVIDE(SUM(c.cost_micros) / 1000000, SUM(c.clicks)) as cpc
FROM `votre-project-id.google_ads.Campaign_XXXXXXXX` c
WHERE c.segments_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
  AND c.campaign_status = 'ENABLED'
GROUP BY c.campaign_name
ORDER BY total_cost_usd DESC;
```

**Exemple de requÃªte - Performance mots-clÃ©s :**

```sql
SELECT
  k.ad_group_criterion_keyword_text as keyword,
  k.segments_date as date,
  SUM(k.impressions) as impressions,
  SUM(k.clicks) as clicks,
  SUM(k.cost_micros) / 1000000 as cost_usd,
  SAFE_DIVIDE(SUM(k.clicks), SUM(k.impressions)) * 100 as ctr
FROM `votre-project-id.google_ads.Keyword_XXXXXXXX` k
WHERE k.segments_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
GROUP BY keyword, date
ORDER BY cost_usd DESC
LIMIT 50;
```

---

#### 7.4 - RÃ©sumÃ© des datasets crÃ©Ã©s

AprÃ¨s configuration, vous aurez ces datasets dans BigQuery :

```
votre-project-id
â”œâ”€â”€ linkedin                    # Scripts Python (ce projet)
â”œâ”€â”€ microsoft_clarity           # Scripts Python (ce projet)
â”œâ”€â”€ spyfu                       # Scripts Python (ce projet)
â”œâ”€â”€ google_search_console       # ğŸ”— Connexion native GSC
â”œâ”€â”€ analytics_XXXXXXXXX         # ğŸ”— Connexion native GA4
â””â”€â”€ google_ads                  # ğŸ”— Connexion native Google Ads
```

**Avantages des connexions natives :**
- âœ… Aucun code Ã  Ã©crire
- âœ… Synchronisation automatique quotidienne
- âœ… Gratuit (sauf streaming GA4 au-delÃ  de 1M Ã©vÃ©nements)
- âœ… DonnÃ©es historiques disponibles
- âœ… SchÃ©ma maintenu par Google
- âœ… Pas de gestion d'API keys

**Analyse combinÃ©e possible :**

```sql
-- CorrÃ©lation Google Ads + LinkedIn Ads
SELECT
  ga.segments_date as date,
  'Google Ads' as source,
  SUM(ga.impressions) as impressions,
  SUM(ga.clicks) as clicks,
  SUM(ga.cost_micros) / 1000000 as cost
FROM `votre-project-id.google_ads.Campaign_XXXXXXXX` ga
WHERE ga.segments_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
GROUP BY date, source

UNION ALL

SELECT
  DATE(l.date) as date,
  'LinkedIn Ads' as source,
  SUM(l.impressions) as impressions,
  SUM(l.clicks) as clicks,
  SUM(l.costInUsd) as cost
FROM `votre-project-id.linkedin.campaign_analytics` l
WHERE DATE(l.date) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
GROUP BY date, source

ORDER BY date DESC, source;
```

---

### Ã‰tape 8 : VÃ©rifier la configuration

```sql
-- VÃ©rifier que les datasets existent
SELECT schema_name
FROM INFORMATION_SCHEMA.SCHEMATA;

-- VÃ©rifier les tables LinkedIn
SELECT table_name
FROM `linkedin.INFORMATION_SCHEMA.TABLES`;

-- VÃ©rifier les tables Clarity
SELECT table_name
FROM `microsoft_clarity.INFORMATION_SCHEMA.TABLES`;

-- VÃ©rifier les tables SpyFu
SELECT table_name
FROM `spyfu.INFORMATION_SCHEMA.TABLES`;
```

---

## ğŸ” Configuration LinkedIn

### Vue d'ensemble OAuth 2.0

LinkedIn utilise OAuth 2.0 avec un **Refresh Token** qui ne change pas et permet de gÃ©nÃ©rer des **Access Tokens** temporaires (60 jours).

**Flux OAuth :**
```
1. CrÃ©er une App LinkedIn
2. Obtenir Client ID + Client Secret
3. GÃ©nÃ©rer un Authorization Code (via navigateur)
4. Ã‰changer le code contre un Refresh Token
5. Utiliser le Refresh Token dans les scripts
   â†’ Les scripts gÃ©nÃ¨rent automatiquement les Access Tokens
```

---

### Ã‰tape 1 : CrÃ©er une application LinkedIn

1. Aller sur [LinkedIn Developers](https://www.linkedin.com/developers/)
2. Se connecter avec votre compte LinkedIn
3. Cliquer sur **"Create app"**

4. Remplir le formulaire :
   - **App name** : `Marketing Data Collector`
   - **LinkedIn Page** : SÃ©lectionner votre page entreprise
     - âš ï¸ **Vous devez Ãªtre admin de la page**
     - Si pas de page : crÃ©er une page entreprise d'abord
   - **Privacy policy URL** : URL de votre politique de confidentialitÃ©
   - **App logo** : Upload un logo (256x256px minimum)
   - **Legal agreement** : Cocher la case

5. Cliquer sur **"Create app"**

---

### Ã‰tape 2 : Demander l'accÃ¨s aux produits

1. Dans votre app, onglet **"Products"**
2. Demander l'accÃ¨s Ã  :
   - âœ… **Marketing Developer Platform**
   - âœ… **Advertising API**

3. Pour chaque produit :
   - Cliquer sur **"Request access"**
   - Remplir le formulaire de demande
   - Expliquer l'usage : "Automated data collection for internal marketing analytics"

â±ï¸ **DÃ©lai d'approbation :** 1-7 jours ouvrÃ©s

ğŸ“§ **Notification :** Email de confirmation une fois approuvÃ©

---

### Ã‰tape 3 : Configurer OAuth 2.0

1. Onglet **"Auth"**
2. **Redirect URLs** â†’ Cliquer sur le crayon pour Ã©diter
3. Ajouter :
   ```
   http://localhost:8080/callback
   ```
4. Cliquer sur **"Update"**

5. **Noter vos credentials** :
   - **Client ID** : `78...` (long string)
   - **Client Secret** : `WP...` (long string)

âš ï¸ **Ne jamais partager** ces credentials !

---

### Ã‰tape 4 : Obtenir l'Ad Account ID

1. Aller sur [LinkedIn Campaign Manager](https://www.linkedin.com/campaignmanager/)
2. L'URL contient votre Account ID :
   ```
   https://www.linkedin.com/campaignmanager/accounts/503061133/
                                                    ^^^^^^^^^^
                                                    Account ID
   ```
3. **Noter cet ID**

---

### Ã‰tape 5 : GÃ©nÃ©rer le Refresh Token

Voir la documentation complÃ¨te dans [linkedin/README.md](linkedin/README.md) section "Configuration OAuth".

**RÃ©sumÃ© :**
1. Configurer `token_linkedin.py` avec Client ID/Secret
2. ExÃ©cuter le script
3. Autoriser dans le navigateur
4. Copier le code de l'URL
5. Obtenir le Refresh Token

---

## ğŸ” Configuration Microsoft Clarity

### Ã‰tape 1-3 : Configuration du projet

Voir [microsoft_clarity/README.md](microsoft_clarity/README.md) pour les Ã©tapes dÃ©taillÃ©es :
1. CrÃ©er un projet Clarity
2. Installer le tracking code
3. Obtenir Project ID et API Key

---

## ğŸ¯ Configuration SpyFu

### Obtenir l'API Key

Voir [spyfu/README.md](spyfu/README.md) pour les Ã©tapes dÃ©taillÃ©es.

**RÃ©sumÃ© :**
1. Se connecter Ã  SpyFu (abonnement requis)
2. Account Settings â†’ API
3. Copier la **Secret Key**

---

## ğŸš€ DÃ©ploiement et exÃ©cution

### Structure de fichiers finale

```
marketing-data-collection/        # Repository clonÃ© depuis GitHub
â”œâ”€â”€ account-key.json              # Ã€ crÃ©er (Service Account GCP - NE PAS COMMIT)
â”œâ”€â”€ linkedin/
â”‚   â”œâ”€â”€ scripts/                  # 4 scripts Python
â”‚   â”œâ”€â”€ sql/                      # 3 fichiers SQL
â”‚   â”œâ”€â”€ data/                     # JSON backups (crÃ©Ã© automatiquement)
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ microsoft_clarity/
â”‚   â”œâ”€â”€ scripts/                  # 1 script Python
â”‚   â”œâ”€â”€ sql/                      # 1 fichier SQL
â”‚   â”œâ”€â”€ data/                     # JSON backups (crÃ©Ã© automatiquement)
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ spyfu/
â”‚   â”œâ”€â”€ scripts/                  # 8 scripts Python
â”‚   â”œâ”€â”€ sql/                      # 1 fichier SQL
â”‚   â”œâ”€â”€ data/                     # JSON backups (crÃ©Ã© automatiquement)
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ SETUP_GUIDE.md                # Ce fichier
â”œâ”€â”€ README.md                     # Documentation principale
â””â”€â”€ .gitignore                    # ProtÃ¨ge les credentials
```

---

### ExÃ©cution manuelle

```bash
# Se placer dans le dossier du repository clonÃ©
cd marketing-data-collection

# LinkedIn
cd linkedin/scripts
python linkedin_campaign_analytics.py
python linkedin_budget.py
python linkedin_lead_forms.py

# Clarity
cd ../../microsoft_clarity/scripts
python clarity_analytics.py

# SpyFu
cd ../../spyfu/scripts
python spyfu_ppc_keywords.py
python spyfu_seo_keywords.py
# ... autres scripts
```

---

## ğŸ”’ SÃ©curitÃ© et bonnes pratiques

### .gitignore (dÃ©jÃ  inclus dans le repository)

Le repository contient dÃ©jÃ  un `.gitignore` qui protÃ¨ge automatiquement :

```gitignore
# Credentials Google Cloud
account-key.json
*.json

# Tokens
*_token.txt
.env

# Data exports (JSON backups)
linkedin/data/*.json
microsoft_clarity/data/*.json
spyfu/data/*.json

# Python
__pycache__/
*.pyc
venv/
.venv/

# Logs
*.log
```

âš ï¸ **Ne jamais modifier le .gitignore pour Ã©viter de commit des credentials !**

---

### Permissions fichiers

```bash
# SÃ©curiser le Service Account key
chmod 600 account-key.json

# Scripts exÃ©cutables
chmod 755 linkedin/scripts/*.py
chmod 755 microsoft_clarity/scripts/*.py
chmod 755 spyfu/scripts/*.py
```

---

## â° Automatisation

### FrÃ©quences recommandÃ©es

| Source | Script | FrÃ©quence | Raison |
|--------|--------|-----------|--------|
| Clarity | clarity_analytics | **Quotidien (2h)** | **API limitÃ©e Ã  3 jours** |
| LinkedIn | campaign_analytics | Quotidien (3h) | MÃ©triques jour J-1 |
| LinkedIn | lead_forms | 2x/jour (8h, 18h) | RÃ©activitÃ© sur leads |
| SpyFu | ppc/seo_keywords | Hebdomadaire | Ã‰conomiser API calls |

### Cron jobs

```bash
crontab -e
```

Ajouter :

```bash
# Variables (adapter selon votre installation)
REPO_DIR=/home/user/marketing-data-collection
PYTHON=/usr/bin/python3  # ou le chemin vers votre venv

# Clarity - QUOTIDIEN OBLIGATOIRE
0 2 * * * cd $REPO_DIR/microsoft_clarity/scripts && $PYTHON clarity_analytics.py >> /var/log/clarity.log 2>&1

# LinkedIn Analytics - Quotidien
0 3 * * * cd $REPO_DIR/linkedin/scripts && $PYTHON linkedin_campaign_analytics.py >> /var/log/linkedin.log 2>&1

# LinkedIn Leads - 2x/jour
0 8,18 * * * cd $REPO_DIR/linkedin/scripts && $PYTHON linkedin_lead_forms.py >> /var/log/linkedin_leads.log 2>&1

# SpyFu - Hebdomadaire (dimanche)
0 5 * * 0 cd $REPO_DIR/spyfu/scripts && $PYTHON spyfu_ppc_keywords.py >> /var/log/spyfu.log 2>&1
```

---

## âš™ï¸ Configuration dÃ©taillÃ©e

### Fichier config.yaml

Le fichier `config.yaml` centralise tous les paramÃ¨tres du projet. Voici comment le remplir section par section :

#### 1. Google Cloud & BigQuery

```yaml
google_cloud:
  project_id: "votre-project-id"  # ID de votre projet GCP
  credentials_file: "./account-key.json"  # Chemin vers le Service Account

  datasets:
    linkedin: "linkedin"  # Nom du dataset pour LinkedIn
    clarity: "microsoft_clarity"  # Nom du dataset pour Clarity
    spyfu: "spyfu"  # Nom du dataset pour SpyFu

  location: "EU"  # EU, US, ou autre rÃ©gion
```

#### 2. LinkedIn

```yaml
linkedin:
  oauth:
    client_id: "VOTRE_CLIENT_ID"  # Depuis LinkedIn App
    client_secret: "VOTRE_CLIENT_SECRET"
    refresh_token: "VOTRE_REFRESH_TOKEN"  # Token permanent

  account_id: "503061133"  # Votre Ad Account ID

  collection:
    start_date: "2024-01-01"  # Date de dÃ©but de collecte
    end_date: null  # null = aujourd'hui
    granularity: "DAILY"  # DAILY, MONTHLY, YEARLY, ALL
    api_version: "202509"  # Version API LinkedIn

  analytics:
    pivots:
      - "CAMPAIGN"  # DonnÃ©es par campagne
      - "CREATIVE"  # DonnÃ©es par creative
```

#### 3. Microsoft Clarity

```yaml
microsoft_clarity:
  project_id: "neutnvggsv"  # Project ID Clarity
  api_key: "VOTRE_API_KEY"  # Token JWT depuis Clarity

  api:
    base_url: "https://www.clarity.ms/api/project-live-insights"
    num_of_days: 1  # 1, 2 ou 3 maximum (limitation API)
```

âš ï¸ **Important:** L'API Clarity limite Ã  3 jours maximum. Collecte quotidienne OBLIGATOIRE.

#### 4. SpyFu

```yaml
spyfu:
  api_key: "VOTRE_SECRET_KEY"  # Secret Key (pas l'ID !)

  global:
    country_code: "FR"  # FR, US, DE, GB, etc.
    page_size: 1000  # Nombre de rÃ©sultats par requÃªte

  domains:
    primary: "votre-domaine.com"  # Votre domaine principal

    competitors:  # Liste de concurrents Ã  surveiller
      - "concurrent1.com"
      - "concurrent2.com"
      - "concurrent3.com"

  comparisons:  # Comparaisons SEO (outrank_comparison)
    - domain: "votre-domaine.com"
      compare_domain: "concurrent1.com"

    - domain: "votre-domaine.com"
      compare_domain: "concurrent2.com"

  filters:  # Filtres globaux
    min_search_volume: 100  # Volume de recherche minimum
    max_keyword_difficulty: 80  # DifficultÃ© maximum
    max_cost_per_click: 50.0  # CPC maximum

  # Configuration par endpoint
  ppc_keywords:
    enabled: true
    sort_by: "SearchVolume"
    filters:
      min_search_volume: 100

  seo_keywords:
    enabled: true
    search_type: "MostValuable"  # MostValuable, GainedClicks, etc.
    filters:
      min_search_volume: 100
      min_seo_clicks: 10
```

**Endpoints disponibles:**
- `ppc_keywords` - Mots-clÃ©s PPC
- `new_keywords` - Nouveaux mots-clÃ©s
- `paid_serps` - SERPs payants
- `seo_keywords` - Mots-clÃ©s SEO
- `newly_ranked` - Nouveaux rankings
- `outrank_comparison` - Comparaisons de ranking
- `top_pages` - Pages les plus performantes
- `ppc_competitors` - Concurrents PPC

#### 5. Automatisation

```yaml
automation:
  schedules:
    linkedin:
      campaign_analytics: "daily"  # Quotidien Ã  3h
      budget: "weekly"  # Hebdomadaire
      lead_forms: "twice_daily"  # 2x/jour

    clarity:
      analytics: "daily"  # OBLIGATOIRE quotidien

    spyfu:
      ppc_keywords: "weekly"  # Hebdomadaire
      seo_keywords: "weekly"
      other: "monthly"  # Autres endpoints
```

#### 6. DÃ©veloppement & Debug

```yaml
development:
  debug_mode: false  # Mode debug (verbose)
  dry_run: false  # Test sans upload BigQuery
  limit_results: null  # Limiter rÃ©sultats (ex: 10 pour tests)
  verbose: false  # Logging dÃ©taillÃ©
```

### Utilisation dans les scripts

Les scripts chargent automatiquement la configuration :

```python
from config_loader import load_config

# Charger la config
config = load_config()

# AccÃ©der aux valeurs
linkedin_config = config.get_linkedin_config()
spyfu_config = config.get_spyfu_config()

# Ou accÃ¨s direct par chemin
project_id = config.get('google_cloud.project_id')
domains = config.get('spyfu.domains.all')
```

### Validation de la configuration

```bash
# Tester la configuration
python config_loader.py

# Affiche :
# âœ“ Configuration chargÃ©e depuis config.yaml
# Configuration Summary
# ======================================
# Google Cloud: clean-avatar-466709-a0
# LinkedIn: 503061133
# SpyFu: votre-domaine.com (3 competitors)
# ...
```

---

## ğŸ”§ Troubleshooting global

### ProblÃ¨mes courants

#### Permission denied

```bash
chmod 600 account-key.json
```

#### Module not found

```bash
pip install pandas-gbq google-auth
```

#### BigQuery access denied

VÃ©rifier les rÃ´les du Service Account :
- BigQuery Data Editor
- BigQuery Job User

#### API rate limiting

Attendre et espacer les requÃªtes.

---

## âœ… Checklist de dÃ©ploiement

### Google Cloud

- [ ] Projet GCP crÃ©Ã©
- [ ] BigQuery API activÃ©e
- [ ] Service Account crÃ©Ã© avec permissions
- [ ] ClÃ© JSON tÃ©lÃ©chargÃ©e et sÃ©curisÃ©e
- [ ] 3 datasets crÃ©Ã©s
- [ ] Tables crÃ©Ã©es depuis SQL

### LinkedIn

- [ ] App LinkedIn crÃ©Ã©e
- [ ] Marketing Developer Platform approuvÃ©
- [ ] Refresh Token gÃ©nÃ©rÃ©
- [ ] Ad Account ID rÃ©cupÃ©rÃ©
- [ ] Scripts configurÃ©s

### Clarity

- [ ] Projet Clarity crÃ©Ã©
- [ ] Tracking code installÃ©
- [ ] API Key rÃ©cupÃ©rÃ©e
- [ ] Script configurÃ©

### SpyFu

- [ ] API Secret Key rÃ©cupÃ©rÃ©e
- [ ] Domaines dÃ©finis
- [ ] Scripts configurÃ©s

### Tests

- [ ] Test LinkedIn analytics
- [ ] Test Clarity analytics
- [ ] Test SpyFu keywords
- [ ] VÃ©rification donnÃ©es BigQuery
- [ ] VÃ©rification JSON backups

### Automatisation

- [ ] Cron jobs configurÃ©s
- [ ] Logs directory crÃ©Ã©
- [ ] Monitoring mis en place

### SÃ©curitÃ©

- [ ] .gitignore configurÃ©
- [ ] Permissions fichiers correctes
- [ ] Credentials non commitÃ©s

---

## ğŸ“š Documentation dÃ©taillÃ©e

Pour plus de dÃ©tails sur chaque source :

- **[LinkedIn README](linkedin/README.md)** - Configuration OAuth complÃ¨te, 3 scripts, 7 tables
- **[Microsoft Clarity README](microsoft_clarity/README.md)** - MÃ©triques UX, interprÃ©tation, scores
- **[SpyFu README](spyfu/README.md)** - 8 endpoints, analyse SEO/PPC, 26 vues

---

### Documentation API

- [LinkedIn Marketing API](https://learn.microsoft.com/en-us/linkedin/marketing/)
- [Microsoft Clarity API](https://docs.microsoft.com/en-us/clarity/)
- [SpyFu API](https://www.spyfu.com/api)
- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)

---

**Version:** 1.0
**DerniÃ¨re mise Ã  jour:** 2025-10-21
**Auteur:** Deep Scouting
