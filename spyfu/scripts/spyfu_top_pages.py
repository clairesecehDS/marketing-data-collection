#!/usr/bin/env python3
"""
SpyFu Top Pages Collector
R√©cup√®re les pages les plus performantes en SEO pour une liste de domaines
"""

import os
import sys
import json
import requests
import time
from datetime import datetime
from typing import List, Dict, Optional
import pandas as pd
import pandas_gbq
from google.oauth2 import service_account
# Ajouter le r√©pertoire parent au path pour importer config_loader
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))
from config_loader import load_config



class SpyFuTopPagesCollector:
    """Collecteur des meilleures pages SEO depuis l'API SpyFu"""

    BASE_URL = "https://api.spyfu.com/apis/serp_api/v2/seo"



    # Types de recherche disponibles
    SEARCH_TYPES = [
        "MostTraffic",  # Pages g√©n√©rant le plus de trafic organique
        "New"           # Pages r√©cemment d√©couvertes avec trafic significatif
    ]

    # Param√®tres par d√©faut
    DEFAULT_PARAMS = {
        "searchType": "MostTraffic",
        "countryCode": "FR",
        "pageSize": 1000,
        "startingRow": 1,
        "sortBy": "SeoClicks",
        "sortOrder": "Descending"
    }

    def __init__(self, api_key: str):
        """
        Initialise le collecteur SpyFu

        Args:
            api_key: Cl√© API SpyFu (Secret Key)
        """
        self.api_key = api_key
        self.session = requests.Session()

    def get_top_pages(
        self,
        domain: str,
        search_type: str = "MostTraffic",
        country_code: str = "FR",
        page_size: int = 1000,
        min_seo_clicks: Optional[int] = None,
        keyword_filter: Optional[str] = None,
        sort_by: str = "SeoClicks",
        max_retries: int = 3
    ) -> List[Dict]:
        """
        R√©cup√®re les pages les plus performantes en SEO pour un domaine

        Args:
            domain: Domaine √† analyser
            search_type: Type de recherche (MostTraffic ou New) - OBLIGATOIRE
            country_code: Code pays (US, DE, GB, FR, etc.)
            page_size: Nombre de r√©sultats par page (max 1000)
            min_seo_clicks: Nombre minimum de clics SEO mensuels
            keyword_filter: Filtre sur les mots-cl√©s (optionnel)
            sort_by: Tri (SeoClicks est le seul support√©)
            max_retries: Nombre maximum de tentatives en cas de rate limit

        Returns:
            Liste des pages avec leurs m√©triques SEO
        """
        endpoint = f"{self.BASE_URL}/getTopPages"

        params = {
            "query": domain,
            "searchType": search_type,  # OBLIGATOIRE selon la doc
            "countryCode": country_code,
            "pageSize": min(page_size, 1000),  # Max 1000 selon la doc
            "startingRow": 1,
            "sortBy": sort_by,
            "sortOrder": "Descending",
            "api_key": self.api_key
        }

        # Filtres optionnels
        if min_seo_clicks:
            params["seoClicks.min"] = min_seo_clicks
        if keyword_filter:
            params["keywordFilter"] = keyword_filter

        headers = {
            "Accept": "application/json"
        }

        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"üîÑ Tentative {attempt + 1}/{max_retries} pour {domain}...")
                else:
                    print(f"üìÑ R√©cup√©ration des top pages pour {domain}...")

                response = self.session.get(endpoint, params=params, headers=headers, timeout=60)
                response.raise_for_status()

                data = response.json()
                pages = data.get("results", [])

                print(f"‚úì {len(pages)} pages r√©cup√©r√©es pour {domain}")
                return pages

            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429:
                    # Rate limit atteint
                    retry_after = 2  # D√©lai par d√©faut
                    try:
                        error_data = e.response.json()
                        retry_after = error_data.get("retryAfter", 2)
                    except:
                        pass

                    if attempt < max_retries - 1:
                        print(f"‚è≥ Rate limit atteint pour {domain}. Attente de {retry_after}s avant nouvelle tentative...")
                        time.sleep(retry_after)
                        continue
                    else:
                        print(f"‚úó Rate limit atteint pour {domain} apr√®s {max_retries} tentatives")
                        return []
                else:
                    print(f"‚úó Erreur HTTP {e.response.status_code} pour {domain}: {e}")
                    if hasattr(e, 'response') and e.response is not None:
                        print(f"  D√©tails: {e.response.text}")
                    return []

            except requests.exceptions.RequestException as e:
                print(f"‚úó Erreur API pour {domain}: {e}")
                if hasattr(e, 'response') and e.response is not None:
                    print(f"  D√©tails: {e.response.text}")
                return []

        return []

    def parse_page_data(self, page_data: Dict, domain: str, country_code: str) -> Dict:
        """
        Parse les donn√©es d'une page au format BigQuery

        Args:
            page_data: Donn√©es brutes de la page depuis l'API
            domain: Domaine analys√©
            country_code: Code pays

        Returns:
            Dictionnaire format√© pour BigQuery
        """
        return {
            # Identifiants
            "domain": domain,
            "url": page_data.get("url"),
            "title": page_data.get("title"),

            # M√©triques de la page
            "keyword_count": page_data.get("keywordCount"),
            "est_monthly_seo_clicks": page_data.get("estMonthlySeoClicks"),

            # Top keyword de la page
            "top_keyword": page_data.get("topKeyword"),
            "top_keyword_position": page_data.get("topKeywordPosition"),
            "top_keyword_search_volume": page_data.get("topKeywordSearchVolume"),
            "top_keyword_clicks": page_data.get("topKeywordClicks"),

            # M√©tadonn√©es
            "country_code": country_code,
            "retrieved_at": datetime.now()
        }

    def collect_all_domains(
        self,
        domains: List[str],
        search_type: str = "MostTraffic",
        country_code: str = "FR",
        min_seo_clicks: Optional[int] = None,
        delay_between_domains: float = 1.5
    ) -> List[Dict]:
        """
        Collecte les donn√©es pour tous les domaines

        Args:
            domains: Liste des domaines √† analyser
            search_type: Type de recherche (MostTraffic ou New)
            country_code: Code pays
            min_seo_clicks: Nombre minimum de clics SEO mensuels
            delay_between_domains: D√©lai en secondes entre chaque domaine

        Returns:
            Liste de toutes les pages format√©es
        """
        if not domains:
            raise ValueError("La liste de domaines ne peut pas √™tre vide")
        all_pages = []

        for i, domain in enumerate(domains):
            # Ajouter un d√©lai entre les domaines (sauf pour le premier)
            if i > 0 and delay_between_domains > 0:
                time.sleep(delay_between_domains)

            raw_pages = self.get_top_pages(
                domain=domain,
                search_type=search_type,
                country_code=country_code,
                min_seo_clicks=min_seo_clicks
            )

            for page in raw_pages:
                parsed = self.parse_page_data(page, domain, country_code)
                all_pages.append(parsed)

        return all_pages

    def export_to_json(self, data: List[Dict], filename: str):
        """Exporte les donn√©es en JSON"""
        if not data:
            print(f"‚ö†Ô∏è  Aucune donn√©e √† exporter")
            return

        filepath = f"../data/{filename}"
        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, default=str)

        print(f"‚úì Donn√©es export√©es: {filepath}")

    def load_from_json(self, filename: str) -> List[Dict]:
        """
        Charge les donn√©es depuis un fichier JSON

        Args:
            filename: Nom du fichier JSON (dans ../data/)

        Returns:
            Liste des donn√©es
        """
        filepath = f"../data/{filename}"

        if not os.path.exists(filepath):
            print(f"‚úó Fichier non trouv√©: {filepath}")
            return []

        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"‚úì {len(data)} lignes charg√©es depuis {filepath}")
        return data

    def upload_to_bigquery(
        self,
        data: List[Dict],
        project_id: str,
        dataset_id: str = "spyfu",
        table_id: str = "top_pages",
        credentials_path: str = "../../account-key.json"
    ):
        """
        Upload les donn√©es vers BigQuery

        Args:
            data: Donn√©es √† uploader
            project_id: ID du projet GCP
            dataset_id: ID du dataset BigQuery
            table_id: ID de la table
            credentials_path: Chemin vers les credentials GCP
        """
        if not data:
            print(f"‚ö†Ô∏è  Aucune donn√©e √† uploader")
            return

        try:
            # Pr√©parer les credentials
            credentials = service_account.Credentials.from_service_account_file(
                credentials_path,
                scopes=["https://www.googleapis.com/auth/bigquery"]
            )

            # Convertir en DataFrame
            df = pd.DataFrame(data)

            # Filtrer uniquement les lignes avec domain NULL (requis par le sch√©ma)
            initial_count = len(df)
            df = df.dropna(subset=['domain'])
            filtered_count = initial_count - len(df)

            if filtered_count > 0:
                print(f"‚ö†Ô∏è  {filtered_count} ligne(s) filtr√©e(s) (champ domain null)")

            if len(df) == 0:
                print(f"‚ö†Ô∏è  Aucune donn√©e valide √† uploader apr√®s filtrage")
                return

            # Conversion des types pour BigQuery
            import json
            for col in df.columns:
                if df[col].dtype == 'object':
                    # V√©rifier si la colonne contient des listes ou dicts
                    sample = df[col].dropna().iloc[0] if len(df[col].dropna()) > 0 else None
                    if isinstance(sample, (list, dict)):
                        # Convertir en JSON string
                        df[col] = df[col].apply(lambda x: json.dumps(x) if isinstance(x, (list, dict)) else x)
                        df[col] = df[col].astype(str)
                    else:
                        # Pour les autres colonnes object, essayer de convertir en num√©rique
                        try:
                            df[col] = pd.to_numeric(df[col])
                        except (ValueError, TypeError):
                            # Garder comme string si conversion √©choue
                            df[col] = df[col].astype(str)
                            # Remplacer 'None' string par None r√©el
                            df[col] = df[col].replace('None', None)

            # G√©rer les colonnes datetime
            if 'retrieved_at' in df.columns:
                df['retrieved_at'] = pd.to_datetime(df['retrieved_at'], utc=True)

            table_full_id = f"{project_id}.{dataset_id}.{table_id}"

            print(f"üì§ Upload de {len(df)} lignes vers {table_full_id}...")

            pandas_gbq.to_gbq(
                df,
                destination_table=f"{dataset_id}.{table_id}",
                project_id=project_id,
                credentials=credentials,
                if_exists='append',
                progress_bar=False
            )

            print(f"‚úì Upload r√©ussi vers BigQuery")

        except Exception as e:
            print(f"‚úó Erreur lors de l'upload BigQuery: {e}")


def main():
    """Point d'entr√©e principal"""
    import sys

    # ============================================================
    # CHARGEMENT DE LA CONFIGURATION
    # ============================================================
    print("üìã Chargement de la configuration...")
    config = load_config()

    # R√©cup√©rer les configurations
    spyfu_config = config.get_spyfu_config()
    google_config = config.get_google_cloud_config()

    API_KEY = spyfu_config['api_key']
    PROJECT_ID = google_config['project_id']
    DATASET_ID = google_config["datasets"]["spyfu"]
    CREDENTIALS_PATH = google_config["credentials_file"]

    # Configuration top_pages
    specific_config = spyfu_config.get('top_pages', {})
    if not specific_config.get('enabled', True):
        print("‚ö†Ô∏è  top_pages d√©sactiv√© dans la configuration")
        return


    # Mode: "collect" ou "upload"
    mode = sys.argv[1] if len(sys.argv) > 1 else "collect"

    if mode == "upload":
        # Mode upload depuis JSON existant
        if len(sys.argv) < 3:
            print("Usage: python spyfu_top_pages.py upload <json_filename>")
            print("Exemple: python spyfu_top_pages.py upload spyfu_top_pages_20250114_123456.json")
            sys.exit(1)

        json_filename = sys.argv[2]

        print("=" * 60)
        print("SpyFu Top Pages - Upload depuis JSON")
        print("=" * 60)

        collector = SpyFuTopPagesCollector(api_key=API_KEY)

        # Charger depuis JSON
        pages_data = collector.load_from_json(json_filename)

        if pages_data:
            # Upload vers BigQuery
            collector.upload_to_bigquery(
                data=pages_data,
                project_id=PROJECT_ID,
                dataset_id=DATASET_ID,
                credentials_path=CREDENTIALS_PATH
            )
            print("\n‚úì Upload termin√©")
        else:
            print("\n‚úó Aucune donn√©e √† uploader")

    else:
        # Mode collection normal
        # Domaines √† analyser (√† personnaliser)
        DOMAINS = spyfu_config["domains"]["all"]

        # Initialiser le collecteur
        collector = SpyFuTopPagesCollector(api_key=API_KEY)

        # Collecter les donn√©es
        print("=" * 60)
        print("SpyFu Top Pages Collection")
        print("=" * 60)

        pages_data = collector.collect_all_domains(
            domains=DOMAINS,
            search_type="MostTraffic",  # "MostTraffic" ou "New"
            country_code="FR",
            min_seo_clicks=None  # Pas de filtre minimum pour avoir tous les r√©sultats
        )

        print(f"\n‚úì Total: {len(pages_data)} pages collect√©es")

        # Exporter en JSON (TOUJOURS avant BigQuery)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_filename = f"spyfu_top_pages_{timestamp}.json"
        collector.export_to_json(pages_data, json_filename)

        # Demander confirmation avant upload BigQuery
        print(f"\n‚úì Donn√©es sauvegard√©es: ../data/{json_filename}")

        # Upload automatique vers BigQuery
        print("\nüì§ Upload vers BigQuery...")
        collector.upload_to_bigquery(
            data=pages_data,
            project_id=PROJECT_ID,
                dataset_id=DATASET_ID,
                credentials_path=CREDENTIALS_PATH
        )
        print("\n‚úì Collection et upload termin√©s")


if __name__ == "__main__":
    main()
