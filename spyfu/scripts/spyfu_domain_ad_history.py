#!/usr/bin/env python3
"""
SpyFu Domain Ad History Collector
R√©cup√®re l'historique des annonces publicitaires pour une liste de domaines
"""

import os
import sys
import json
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import pandas as pd
import pandas_gbq
from google.oauth2 import service_account

# Ajouter le r√©pertoire parent au path pour importer config_loader
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))
from config_loader import load_config


class SpyFuDomainAdHistoryCollector:
    """Collecteur d'historique des annonces depuis l'API SpyFu"""

    BASE_URL = "https://api.spyfu.com/apis/cloud_ad_history_api/v2/domain"

    def __init__(self, api_key: str):
        """
        Initialise le collecteur SpyFu

        Args:
            api_key: Cl√© API SpyFu (Secret Key)
        """
        self.api_key = api_key
        self.session = requests.Session()

    def get_domain_ad_history(
        self,
        domain: str,
        country_code: str = "US",
        rowcount: int = 23,
        min_date: Optional[str] = None,
        max_date: Optional[str] = None
    ) -> List[Dict]:
        """
        R√©cup√®re l'historique des annonces pour un domaine

        Args:
            domain: Domaine √† analyser
            country_code: Code pays (US, FR, GB, etc.)
            rowcount: Nombre de r√©sultats (max 23 selon budget)
            min_date: Date minimale (format YYYY-MM-DD)
            max_date: Date maximale (format YYYY-MM-DD)

        Returns:
            Liste des annonces avec leurs m√©triques
        """
        endpoint = f"{self.BASE_URL}/getDomainAdHistory"

        # Par d√©faut: 3 derniers mois si non sp√©cifi√©
        if not max_date:
            max_date = datetime.now().strftime("%Y-%m-%d")
        if not min_date:
            min_date = (datetime.now() - timedelta(days=90)).strftime("%Y-%m-%d")

        params = {
            "domain": domain,
            "countryCode": country_code,
            "rowcount": rowcount,
            "minDate": min_date,
            "maxDate": max_date,
            "api_key": self.api_key
        }

        headers = {
            "Accept": "application/json"
        }

        try:
            print(f"üìú R√©cup√©ration de l'historique des annonces pour {domain} ({min_date} √† {max_date})...")
            response = self.session.get(endpoint, params=params, headers=headers, timeout=60)
            response.raise_for_status()

            data = response.json()
            ads = data.get("results", [])

            print(f"‚úì {len(ads)} annonces r√©cup√©r√©es pour {domain}")
            return ads

        except requests.exceptions.RequestException as e:
            print(f"‚úó Erreur API pour {domain}: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"  D√©tails: {e.response.text}")
            return []

    def get_keyword_metrics(
        self,
        domain: str,
        keyword: str,
        country_code: str = "US"
    ) -> Dict:
        """
        R√©cup√®re les m√©triques d'un keyword sp√©cifique via l'API PPC Keywords

        Args:
            domain: Domaine analys√©
            keyword: Mot-cl√© √† rechercher
            country_code: Code pays

        Returns:
            Dict avec search_volume, cost_per_click, monthly_cost (ou None si non trouv√©)
        """
        # V√©rifier le cache
        cache_key = f"{domain}:{keyword}:{country_code}"
        if not hasattr(self, '_keyword_cache'):
            self._keyword_cache = {}

        if cache_key in self._keyword_cache:
            return self._keyword_cache[cache_key]

        # Appel API
        endpoint = "https://api.spyfu.com/apis/keyword_api/v2/ppc/getMostSuccessful"

        params = {
            "query": domain,
            "countryCode": country_code,
            "pageSize": 20,
            "startingRow": 1,
            "sortBy": "SearchVolume",
            "sortOrder": "Descending",
            "api_key": self.api_key
        }

        headers = {"Accept": "application/json"}

        try:
            response = self.session.get(endpoint, params=params, headers=headers, timeout=60)
            response.raise_for_status()

            data = response.json()
            keywords_data = data.get("results", [])

            # Chercher le keyword sp√©cifique (insensible √† la casse)
            for kw_data in keywords_data:
                if kw_data.get("keyword", "").lower() == keyword.lower():
                    metrics = {
                        "search_volume": kw_data.get("searchVolume"),
                        "cost_per_click": kw_data.get("broadCostPerClick"),
                        "monthly_cost": kw_data.get("broadMonthlyCost")
                    }
                    self._keyword_cache[cache_key] = metrics
                    return metrics

            # Si non trouv√© dans les r√©sultats
            default_metrics = {"search_volume": None, "cost_per_click": None, "monthly_cost": None}
            self._keyword_cache[cache_key] = default_metrics
            return default_metrics

        except requests.exceptions.RequestException as e:
            print(f"  ‚ö†Ô∏è  Erreur lors de la r√©cup√©ration des m√©triques pour '{keyword}': {e}")
            return {"search_volume": None, "cost_per_click": None, "monthly_cost": None}

    def parse_ad_data(self, ad_data: Dict, domain: str, country_code: str, enrich: bool = True) -> Dict:
        """
        Parse les donn√©es d'une annonce au format BigQuery avec enrichissement

        Args:
            ad_data: Donn√©es brutes de l'annonce depuis l'API
            domain: Domaine analys√©
            country_code: Code pays
            enrich: Si True, enrichit avec les m√©triques du keyword (search_volume, CPC, etc.)

        Returns:
            Dictionnaire format√© pour BigQuery
        """
        # Convertir searchDateId en date (format YYYYMMDD)
        search_date_id = ad_data.get("searchDateId")
        first_seen = None
        if search_date_id:
            try:
                date_str = str(search_date_id)
                first_seen = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
            except:
                pass

        # Extraire le premier keyword de la liste
        keywords_list = ad_data.get("keywords", [])
        keyword = keywords_list[0] if keywords_list else None

        # Extraire display_url depuis l'URL de destination
        destination_url = ad_data.get("url")
        display_url = None
        if destination_url:
            try:
                from urllib.parse import urlparse
                parsed = urlparse(destination_url)
                # Format: domain.com/path
                display_url = f"{parsed.netloc}{parsed.path}".rstrip("/")
            except:
                display_url = None

        # Structure de base
        parsed_data = {
            # Identifiants
            "domain": domain,
            "ad_id": str(ad_data.get("adId")) if ad_data.get("adId") else None,
            "keyword": keyword,

            # Contenu de l'annonce (l'API utilise title/body au lieu de headline/description)
            "headline": ad_data.get("title"),
            "description": ad_data.get("body"),
            "display_url": display_url,  # Extrait depuis l'URL
            "destination_url": destination_url,

            # M√©triques temporelles
            "first_seen_date": first_seen,
            # Note: last_seen_date et days_seen ne sont pas dans le sch√©ma BigQuery actuel

            # M√©triques de performance (√† enrichir)
            "search_volume": None,
            "cost_per_click": None,
            "monthly_cost": None,
            "position": ad_data.get("position"),

            # M√©tadonn√©es
            "country_code": country_code,
            "retrieved_at": datetime.now()
        }

        # Enrichissement avec les m√©triques du keyword via API PPC
        if enrich and keyword:
            print(f"    üîç Enrichissement pour keyword '{keyword}'...")
            metrics = self.get_keyword_metrics(domain, keyword, country_code)
            parsed_data["search_volume"] = metrics.get("search_volume")
            parsed_data["cost_per_click"] = metrics.get("cost_per_click")
            parsed_data["monthly_cost"] = metrics.get("monthly_cost")

        return parsed_data

    def collect_all_domains(
        self,
        domains: List[str],
        country_code: str = "US",
        rowcount: int = 23,
        min_date: Optional[str] = None,
        max_date: Optional[str] = None,
        enrich: bool = True
    ) -> List[Dict]:
        """
        Collecte les donn√©es pour tous les domaines

        Args:
            domains: Liste des domaines √† analyser
            country_code: Code pays
            rowcount: Nombre de r√©sultats par domaine
            min_date: Date minimale
            max_date: Date maximale
            enrich: Si True, enrichit avec les m√©triques des keywords (recommand√©)

        Returns:
            Liste de toutes les annonces format√©es
        """
        if not domains:
            raise ValueError("La liste de domaines ne peut pas √™tre vide")

        all_ads = []

        for domain in domains:
            print(f"\n{'='*80}")
            print(f"üìç Traitement de {domain}")
            print(f"{'='*80}")

            raw_ads = self.get_domain_ad_history(
                domain=domain,
                country_code=country_code,
                rowcount=rowcount,
                min_date=min_date,
                max_date=max_date
            )

            for ad in raw_ads:
                parsed = self.parse_ad_data(ad, domain, country_code, enrich)
                all_ads.append(parsed)

            print(f"‚úì {len(raw_ads)} annonces trait√©es pour {domain}")

        return all_ads

    def export_to_json(self, data: List[Dict], filename: str):
        """Exporte les donn√©es en JSON"""
        if not data:
            print(f"‚ö†Ô∏è  Aucune donn√©e √† exporter")
            return

        # Skip export en Cloud Functions
        if os.getenv('FUNCTION_TARGET'):
            return

        filepath = f"../data/{filename}"
        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, default=str)

    def load_from_json(self, filename: str) -> List[Dict]:
        """Charge les donn√©es depuis un fichier JSON"""
        filepath = f"../data/{filename}"

        if not os.path.exists(filepath):
            print(f"‚úó Fichier non trouv√©: {filepath}")
            return []

        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"‚úì {len(data)} lignes charg√©es depuis {filepath}")
        return data

    def upload_to_bigquery(
        self,
        data: List[Dict],
        project_id: str,
        dataset_id: str = "spyfu",
        table_id: str = "domain_ad_history",
        credentials_path: str = "../../account-key.json"
    ):
        """Upload les donn√©es vers BigQuery"""
        if not data:
            print(f"‚ö†Ô∏è  Aucune donn√©e √† uploader")
            return

        try:
            # Pr√©parer les credentials
            if os.getenv('FUNCTION_TARGET'):
                credentials = None
            elif credentials_path and os.path.exists(credentials_path):
                credentials = service_account.Credentials.from_service_account_file(
                    credentials_path,
                    scopes=["https://www.googleapis.com/auth/bigquery"]
                )
            else:
                credentials = None

            # Convertir en DataFrame
            df = pd.DataFrame(data)

            # Filtrer les lignes avec domain NULL
            initial_count = len(df)
            df = df.dropna(subset=['domain'])
            filtered_count = initial_count - len(df)

            if filtered_count > 0:
                print(f"‚ö†Ô∏è  {filtered_count} ligne(s) filtr√©e(s) (champ domain null)")

            if len(df) == 0:
                print(f"‚ö†Ô∏è  Aucune donn√©e valide √† uploader apr√®s filtrage")
                return

            # Conversion des types pour BigQuery
            for col in df.columns:
                if df[col].dtype == 'object' and col not in ['first_seen_date']:
                    try:
                        df[col] = pd.to_numeric(df[col])
                    except (ValueError, TypeError):
                        df[col] = df[col].astype(str)
                        df[col] = df[col].replace('None', None)

            # G√©rer les colonnes datetime
            date_columns = ['retrieved_at', 'first_seen_date']
            for col in date_columns:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col], utc=True, errors='coerce')

            table_full_id = f"{project_id}.{dataset_id}.{table_id}"

            print(f"üì§ Upload de {len(df)} lignes vers {table_full_id}...")

            pandas_gbq.to_gbq(
                df,
                destination_table=f"{dataset_id}.{table_id}",
                project_id=project_id,
                credentials=credentials,
                if_exists='append',
                progress_bar=False
            )

            print(f"‚úì Upload r√©ussi vers BigQuery")

        except Exception as e:
            print(f"‚úó Erreur lors de l'upload BigQuery: {e}")
            import traceback
            traceback.print_exc()


def main():
    """Point d'entr√©e principal"""

    # Charger la configuration
    is_cloud_function = os.getenv('FUNCTION_TARGET') is not None
    config = load_config(skip_credentials_check=is_cloud_function)

    # R√©cup√©rer les configurations
    spyfu_config = config.get_spyfu_config()
    google_config = config.get_google_cloud_config()

    API_KEY = spyfu_config['api_key']
    PROJECT_ID = google_config['project_id']
    DATASET_ID = google_config['datasets']['spyfu']
    CREDENTIALS_PATH = google_config['credentials_file']
    # Essayer d'abord spyfu.global.country_code, puis spyfu.country_code, par d√©faut US
    COUNTRY_CODE = spyfu_config.get('global', {}).get('country_code') or spyfu_config.get('country_code', 'US')

    # Param√®tres depuis le fichier .odt
    ROWCOUNT = 23  # Selon le document .odt

    # Dates: 3 derniers mois par d√©faut
    MAX_DATE = datetime.now().strftime("%Y-%m-%d")
    MIN_DATE = (datetime.now() - timedelta(days=90)).strftime("%Y-%m-%d")

    # Mode: "collect" ou "upload"
    mode = sys.argv[1] if len(sys.argv) > 1 else "collect"

    if mode == "upload":
        if len(sys.argv) < 3:
            print("Usage: python spyfu_domain_ad_history.py upload <json_filename>")
            sys.exit(1)

        json_filename = sys.argv[2]
        print("SpyFu Domain Ad History - Upload depuis JSON")

        collector = SpyFuDomainAdHistoryCollector(api_key=API_KEY)
        ads_data = collector.load_from_json(json_filename)

        if ads_data:
            collector.upload_to_bigquery(
                data=ads_data,
                project_id=PROJECT_ID,
                dataset_id=DATASET_ID,
                credentials_path=CREDENTIALS_PATH
            )
            print("\n‚úì Upload termin√©")
        else:
            print("\n‚úó Aucune donn√©e √† uploader")

    else:
        # Mode collection normal
        DOMAINS = spyfu_config['domains']['all']

        print("=" * 80)
        print("  SpyFu Domain Ad History Collection - VERSION ENRICHIE")
        print("=" * 80)
        print(f"üìç Pays: {COUNTRY_CODE}")
        print(f"üåê Domaines: {', '.join(DOMAINS)}")
        print(f"üìä Rowcount: {ROWCOUNT} par domaine")
        print(f"üìÖ P√©riode: {MIN_DATE} √† {MAX_DATE}")
        print("\nüîç Enrichissement activ√©:")
        print("   ‚úÖ display_url: Extrait depuis l'URL de destination")
        print("   ‚úÖ search_volume, cost_per_click, monthly_cost: Via API PPC Keywords")
        print("=" * 80)

        # Initialiser le collecteur
        collector = SpyFuDomainAdHistoryCollector(api_key=API_KEY)

        # Collecter les donn√©es avec enrichissement
        ads_data = collector.collect_all_domains(
            domains=DOMAINS,
            country_code=COUNTRY_CODE,
            rowcount=ROWCOUNT,
            min_date=MIN_DATE,
            max_date=MAX_DATE,
            enrich=True  # Active l'enrichissement
        )

        print(f"\n‚úì Total: {len(ads_data)} annonces collect√©es")

        # Exporter en JSON
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_filename = f"spyfu_domain_ad_history_{timestamp}.json"
        collector.export_to_json(ads_data, json_filename)
        print(f"‚úì Donn√©es sauvegard√©es: ../data/{json_filename}")

        # Upload vers BigQuery
        print("\nüì§ Upload vers BigQuery...")
        collector.upload_to_bigquery(
            data=ads_data,
            project_id=PROJECT_ID,
            dataset_id=DATASET_ID,
            credentials_path=CREDENTIALS_PATH
        )
        print("\n‚úì Collection et upload termin√©s")


if __name__ == "__main__":
    main()
